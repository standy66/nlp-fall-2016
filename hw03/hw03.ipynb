{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.2\n",
      "  Downloading nltk-3.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |################################| 1.2MB 516kB/s ta 0:00:01K    5% |#                               | 61kB 184kB/s eta 0:00:07\u001b[K    7% |##                              | 92kB 192kB/s eta 0:00:06\u001b[K    9% |##                              | 112kB 206kB/s eta 0:00:06\u001b[K    11% |###                             | 143kB 253kB/s eta 0:00:05\u001b[K    14% |####                            | 174kB 286kB/s eta 0:00:04\u001b[K    16% |#####                           | 194kB 298kB/s eta 0:00:04\u001b[K    18% |#####                           | 225kB 346kB/s eta 0:00:03\u001b[K    20% |######                          | 245kB 317kB/s eta 0:00:04\u001b[K    22% |#######                         | 276kB 387kB/s eta 0:00:03\u001b[K    25% |########                        | 307kB 413kB/s eta 0:00:03\u001b[K    26% |########                        | 327kB 357kB/s eta 0:00:03\u001b[K    29% |#########                       | 358kB 427kB/s eta 0:00:03\u001b[K    31% |#########                       | 378kB 377kB/s eta 0:00:03\u001b[K    32% |##########                      | 399kB 451kB/s eta 0:00:02\u001b[K    34% |###########                     | 419kB 476kB/s eta 0:00:02\u001b[K    36% |###########                     | 440kB 466kB/s eta 0:00:02\u001b[K    38% |############                    | 471kB 493kB/s eta 0:00:02\u001b[K    40% |############                    | 491kB 434kB/s eta 0:00:02\u001b[K    42% |#############                   | 522kB 518kB/s eta 0:00:02\u001b[K    45% |##############                  | 552kB 528kB/s eta 0:00:02\u001b[K    47% |###############                 | 573kB 471kB/s eta 0:00:02\u001b[K    49% |###############                 | 604kB 577kB/s eta 0:00:02\u001b[K    52% |################                | 634kB 598kB/s eta 0:00:01\u001b[K    54% |#################               | 665kB 615kB/s eta 0:00:01\u001b[K    56% |##################              | 686kB 619kB/s eta 0:00:01\u001b[K    58% |##################              | 706kB 536kB/s eta 0:00:01\u001b[K    61% |###################             | 747kB 652kB/s eta 0:00:01\u001b[K    63% |####################            | 768kB 622kB/s eta 0:00:01\u001b[K    64% |####################            | 788kB 554kB/s eta 0:00:01\u001b[K    67% |#####################           | 819kB 568kB/s eta 0:00:01\u001b[K    69% |######################          | 849kB 674kB/s eta 0:00:01\u001b[K    71% |######################          | 870kB 620kB/s eta 0:00:01\u001b[K    74% |#######################         | 901kB 623kB/s eta 0:00:01\u001b[K    76% |########################        | 931kB 729kB/s eta 0:00:01\u001b[K    79% |#########################       | 962kB 771kB/s eta 0:00:01\u001b[K    81% |##########################      | 993kB 732kB/s eta 0:00:01\u001b[K    83% |##########################      | 1.0MB 572kB/s eta 0:00:01\u001b[K    85% |###########################     | 1.0MB 648kB/s eta 0:00:01\u001b[K    87% |############################    | 1.1MB 558kB/s eta 0:00:01\u001b[K    90% |############################    | 1.1MB 609kB/s eta 0:00:01\u001b[K    91% |#############################   | 1.1MB 617kB/s eta 0:00:01\u001b[K    94% |##############################  | 1.1MB 457kB/s eta 0:00:01\u001b[K    96% |############################### | 1.2MB 544kB/s eta 0:00:01\u001b[K    98% |############################### | 1.2MB 503kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/43/dd/39/f0155c88c9188dda43977fbb04c8c0cf9e61ca82dd10135b52\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.1\n",
      "    Uninstalling nltk-3.2.1:\n",
      "      Successfully uninstalled nltk-3.2.1\n",
      "Successfully installed nltk-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk==3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"/root/notebooks/avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print(\"Blocked ratio\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 500000\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "\n",
    "df = pd.concat([df[df.is_blocked == 0][:250000], df[df.is_blocked == 1][:250000]], ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFkCAYAAAD7dJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRJIkVcVwIkmSqmI4kSRJVfl/rMFGvJ8SofEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff69beda6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = filter(lambda key: token_counts[key]>min_count, token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 78707\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print(\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print(\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (500000, 15)\n",
      "Toyota Sera, 1991 -> [37356 67976 36959     0     0     0     0     0     0     0] ...\n",
      "Костюм Steilmann -> [57842     0     0     0     0     0     0     0     0     0] ...\n",
      "Костюм Didriksons Boardman, размер 100, краги, шап -> [57842 47127     0 63330 17544 61330     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print(title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":x, \"subcategory\":y} for x, y in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens, df_non_text, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data (may take up to 3 minutes)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-cefcf985afb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessed_data.pcl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token_to_id.pcl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_tuple' is not defined"
     ]
    }
   ],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print(\"Saving preprocessed data (may take up to 3 minutes)\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print(\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print(\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1070 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=256)\n",
    "descr_nn = lasagne.layers.flatten(descr_nn)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=64)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.flatten(title_nn)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=128)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn, num_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 512)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.5)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y, delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y, delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our good ol'minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375000, 150)\n",
      "(375000, 15)\n",
      "(375000, 67)\n",
      "(375000,)\n"
     ]
    }
   ],
   "source": [
    "print(desc_tr.shape)\n",
    "print(title_tr.shape)\n",
    "print(nontext_tr.shape)\n",
    "print(target_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nontext_tr = nontext_tr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 73.4284375619\n",
      "\tacc: 0.683267326733\n",
      "\tauc: 0.704398752027\n",
      "\tap@k: 0.143597633167\n",
      "Val:\n",
      "\tloss: 117.974640994\n",
      "\tacc: 0.742376237624\n",
      "\tauc: 0.771318992904\n",
      "\tap@k: 0.960209518489\n",
      "Train:\n",
      "\tloss: 278.89449514\n",
      "\tacc: 0.735544554455\n",
      "\tauc: 0.741743417462\n",
      "\tap@k: 0.0789274045319\n",
      "Val:\n",
      "\tloss: 494.231878606\n",
      "\tacc: 0.725841584158\n",
      "\tauc: 0.695182818891\n",
      "\tap@k: 0.719464528506\n",
      "Train:\n",
      "\tloss: 151.499390732\n",
      "\tacc: 0.747623762376\n",
      "\tauc: 0.733899756984\n",
      "\tap@k: 0.100117445008\n",
      "Val:\n",
      "\tloss: 106.439458209\n",
      "\tacc: 0.728316831683\n",
      "\tauc: 0.708141452386\n",
      "\tap@k: 0.767918320816\n",
      "Train:\n",
      "\tloss: 55.3420942356\n",
      "\tacc: 0.750495049505\n",
      "\tauc: 0.768896403971\n",
      "\tap@k: 0.281838291431\n",
      "Val:\n",
      "\tloss: 262.262440985\n",
      "\tacc: 0.734257425743\n",
      "\tauc: 0.765863827426\n",
      "\tap@k: 0.99505396881\n",
      "Train:\n",
      "\tloss: 38.7307689933\n",
      "\tacc: 0.753366336634\n",
      "\tauc: 0.788137711008\n",
      "\tap@k: 0.0888416600877\n",
      "Val:\n",
      "\tloss: 61.846409713\n",
      "\tacc: 0.775346534653\n",
      "\tauc: 0.798103412687\n",
      "\tap@k: 0.997778328188\n",
      "Train:\n",
      "\tloss: 57.5414895119\n",
      "\tacc: 0.778217821782\n",
      "\tauc: 0.814169711882\n",
      "\tap@k: 0.130376374969\n",
      "Val:\n",
      "\tloss: 169.818117769\n",
      "\tacc: 0.749306930693\n",
      "\tauc: 0.816987657295\n",
      "\tap@k: 0.999159802269\n",
      "Train:\n",
      "\tloss: 53.7246901129\n",
      "\tacc: 0.780396039604\n",
      "\tauc: 0.832932774551\n",
      "\tap@k: 0.342762286064\n",
      "Val:\n",
      "\tloss: 85.4868463385\n",
      "\tacc: 0.746930693069\n",
      "\tauc: 0.807729082391\n",
      "\tap@k: 0.957018682528\n",
      "Train:\n",
      "\tloss: 128.280425932\n",
      "\tacc: 0.771089108911\n",
      "\tauc: 0.81372411754\n",
      "\tap@k: 0.255871578648\n",
      "Val:\n",
      "\tloss: 186.228950822\n",
      "\tacc: 0.805247524752\n",
      "\tauc: 0.849459957043\n",
      "\tap@k: 0.894139938861\n",
      "Train:\n",
      "\tloss: 9.03626989326\n",
      "\tacc: 0.782772277228\n",
      "\tauc: 0.840724185806\n",
      "\tap@k: 0.642780779065\n",
      "Val:\n",
      "\tloss: 0.437448413773\n",
      "\tacc: 0.839900990099\n",
      "\tauc: 0.934868725021\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.48742486471\n",
      "\tacc: 0.782475247525\n",
      "\tauc: 0.851622585102\n",
      "\tap@k: 0.962872493609\n",
      "Val:\n",
      "\tloss: 0.406666500462\n",
      "\tacc: 0.847227722772\n",
      "\tauc: 0.932839734936\n",
      "\tap@k: 0.995189113756\n",
      "Train:\n",
      "\tloss: 3.08824787657\n",
      "\tacc: 0.803366336634\n",
      "\tauc: 0.862911857864\n",
      "\tap@k: 0.960102744875\n",
      "Val:\n",
      "\tloss: 0.374175987565\n",
      "\tacc: 0.850693069307\n",
      "\tauc: 0.93860060869\n",
      "\tap@k: 0.997991311922\n",
      "Train:\n",
      "\tloss: 1.2602022275\n",
      "\tacc: 0.801584158416\n",
      "\tauc: 0.863318898334\n",
      "\tap@k: 0.877715716127\n",
      "Val:\n",
      "\tloss: 0.380880067726\n",
      "\tacc: 0.841881188119\n",
      "\tauc: 0.940645455189\n",
      "\tap@k: 0.987677939449\n",
      "Train:\n",
      "\tloss: 0.485760613287\n",
      "\tacc: 0.790693069307\n",
      "\tauc: 0.863875951593\n",
      "\tap@k: 0.9677706766\n",
      "Val:\n",
      "\tloss: 0.349449542118\n",
      "\tacc: 0.851485148515\n",
      "\tauc: 0.946153943597\n",
      "\tap@k: 0.997618743372\n",
      "Train:\n",
      "\tloss: 0.383834176373\n",
      "\tacc: 0.834059405941\n",
      "\tauc: 0.898771770865\n",
      "\tap@k: 0.963764193561\n",
      "Val:\n",
      "\tloss: 0.348581427421\n",
      "\tacc: 0.857128712871\n",
      "\tauc: 0.946105027213\n",
      "\tap@k: 0.994647377556\n",
      "Train:\n",
      "\tloss: 438.716265306\n",
      "\tacc: 0.811683168317\n",
      "\tauc: 0.857529017631\n",
      "\tap@k: 0.578086869922\n",
      "Val:\n",
      "\tloss: 1.61229072412\n",
      "\tacc: 0.856732673267\n",
      "\tauc: 0.94466434871\n",
      "\tap@k: 0.891988529098\n",
      "Train:\n",
      "\tloss: 0.73426300161\n",
      "\tacc: 0.828316831683\n",
      "\tauc: 0.86534927451\n",
      "\tap@k: 0.676608916816\n",
      "Val:\n",
      "\tloss: 0.333940370089\n",
      "\tacc: 0.849702970297\n",
      "\tauc: 0.929634408055\n",
      "\tap@k: 0.89200016\n",
      "Train:\n",
      "\tloss: 0.374533214687\n",
      "\tacc: 0.832475247525\n",
      "\tauc: 0.856721250407\n",
      "\tap@k: 0.826416767144\n",
      "Val:\n",
      "\tloss: 0.298474473623\n",
      "\tacc: 0.871188118812\n",
      "\tauc: 0.92429084631\n",
      "\tap@k: 0.885675190178\n",
      "Train:\n",
      "\tloss: 0.37822021122\n",
      "\tacc: 0.824455445545\n",
      "\tauc: 0.847435492175\n",
      "\tap@k: 0.773549155629\n",
      "Val:\n",
      "\tloss: 0.274216602521\n",
      "\tacc: 0.882772277228\n",
      "\tauc: 0.951007197689\n",
      "\tap@k: 0.956931920721\n",
      "Train:\n",
      "\tloss: 0.359061281457\n",
      "\tacc: 0.839702970297\n",
      "\tauc: 0.860297539612\n",
      "\tap@k: 0.722599496986\n",
      "Val:\n",
      "\tloss: 0.296378575103\n",
      "\tacc: 0.883564356436\n",
      "\tauc: 0.941811173093\n",
      "\tap@k: 0.865637544805\n",
      "Train:\n",
      "\tloss: 0.373936484173\n",
      "\tacc: 0.836633663366\n",
      "\tauc: 0.860676892104\n",
      "\tap@k: 0.778655187215\n",
      "Val:\n",
      "\tloss: 0.284691273381\n",
      "\tacc: 0.858316831683\n",
      "\tauc: 0.941628864687\n",
      "\tap@k: 0.930069727079\n",
      "Train:\n",
      "\tloss: 0.342668348923\n",
      "\tacc: 0.848415841584\n",
      "\tauc: 0.874559876397\n",
      "\tap@k: 0.773665686318\n",
      "Val:\n",
      "\tloss: 0.259300003542\n",
      "\tacc: 0.89198019802\n",
      "\tauc: 0.949524304357\n",
      "\tap@k: 0.9069779129\n",
      "Train:\n",
      "\tloss: 0.337832327615\n",
      "\tacc: 0.849702970297\n",
      "\tauc: 0.865891389782\n",
      "\tap@k: 0.740050993169\n",
      "Val:\n",
      "\tloss: 0.241888065297\n",
      "\tacc: 0.889108910891\n",
      "\tauc: 0.954479317989\n",
      "\tap@k: 0.905638052933\n",
      "Train:\n",
      "\tloss: 6.56241056772\n",
      "\tacc: 0.82099009901\n",
      "\tauc: 0.823915122056\n",
      "\tap@k: 0.530468654242\n",
      "Val:\n",
      "\tloss: 0.255960806042\n",
      "\tacc: 0.884455445545\n",
      "\tauc: 0.906808852984\n",
      "\tap@k: 0.838169362404\n",
      "Train:\n",
      "\tloss: 0.407438661757\n",
      "\tacc: 0.81\n",
      "\tauc: 0.826577790933\n",
      "\tap@k: 0.761088005751\n",
      "Val:\n",
      "\tloss: 0.240572659674\n",
      "\tacc: 0.913168316832\n",
      "\tauc: 0.960341325619\n",
      "\tap@k: 0.93377635639\n",
      "Train:\n",
      "\tloss: 0.43454487669\n",
      "\tacc: 0.801386138614\n",
      "\tauc: 0.814011914676\n",
      "\tap@k: 0.760578507341\n",
      "Val:\n",
      "\tloss: 0.21362477957\n",
      "\tacc: 0.912772277228\n",
      "\tauc: 0.963327271977\n",
      "\tap@k: 0.95348352952\n",
      "Train:\n",
      "\tloss: 0.414961007721\n",
      "\tacc: 0.807920792079\n",
      "\tauc: 0.820078532799\n",
      "\tap@k: 0.683514737252\n",
      "Val:\n",
      "\tloss: 0.209714750825\n",
      "\tacc: 0.908118811881\n",
      "\tauc: 0.941308046952\n",
      "\tap@k: 0.868664102181\n",
      "Train:\n",
      "\tloss: 0.411031874135\n",
      "\tacc: 0.809900990099\n",
      "\tauc: 0.826910570751\n",
      "\tap@k: 0.732315134862\n",
      "Val:\n",
      "\tloss: 0.184312884052\n",
      "\tacc: 0.920693069307\n",
      "\tauc: 0.95219288759\n",
      "\tap@k: 0.893347678698\n",
      "Train:\n",
      "\tloss: 0.419110252344\n",
      "\tacc: 0.805841584158\n",
      "\tauc: 0.815735760365\n",
      "\tap@k: 0.702877907654\n",
      "Val:\n",
      "\tloss: 0.189093142653\n",
      "\tacc: 0.911485148515\n",
      "\tauc: 0.94498997996\n",
      "\tap@k: 0.880091741278\n",
      "Train:\n",
      "\tloss: 0.372490011678\n",
      "\tacc: 0.826732673267\n",
      "\tauc: 0.840992650664\n",
      "\tap@k: 0.756523384928\n",
      "Val:\n",
      "\tloss: 0.175998152217\n",
      "\tacc: 0.925544554455\n",
      "\tauc: 0.951677843716\n",
      "\tap@k: 0.95113702025\n",
      "Train:\n",
      "\tloss: 0.37307422537\n",
      "\tacc: 0.823366336634\n",
      "\tauc: 0.837981624884\n",
      "\tap@k: 0.773378075935\n",
      "Val:\n",
      "\tloss: 0.209251567786\n",
      "\tacc: 0.903069306931\n",
      "\tauc: 0.936031271955\n",
      "\tap@k: 0.94969883308\n",
      "Train:\n",
      "\tloss: 0.385429838831\n",
      "\tacc: 0.822376237624\n",
      "\tauc: 0.832096326251\n",
      "\tap@k: 0.76200429352\n",
      "Val:\n",
      "\tloss: 0.182611362326\n",
      "\tacc: 0.922277227723\n",
      "\tauc: 0.96747905262\n",
      "\tap@k: 0.955184013974\n",
      "Train:\n",
      "\tloss: 0.377269057466\n",
      "\tacc: 0.822277227723\n",
      "\tauc: 0.834501758441\n",
      "\tap@k: 0.754509703625\n",
      "Val:\n",
      "\tloss: 0.170684665318\n",
      "\tacc: 0.924455445545\n",
      "\tauc: 0.956969608347\n",
      "\tap@k: 0.92893550389\n",
      "Train:\n",
      "\tloss: 0.376836635057\n",
      "\tacc: 0.821782178218\n",
      "\tauc: 0.835538219296\n",
      "\tap@k: 0.759310320798\n",
      "Val:\n",
      "\tloss: 0.232150732786\n",
      "\tacc: 0.891683168317\n",
      "\tauc: 0.935653788223\n",
      "\tap@k: 0.856629701935\n",
      "Train:\n",
      "\tloss: 0.41718944677\n",
      "\tacc: 0.821683168317\n",
      "\tauc: 0.841766059747\n",
      "\tap@k: 0.773733941798\n",
      "Val:\n",
      "\tloss: 0.191888178971\n",
      "\tacc: 0.916534653465\n",
      "\tauc: 0.953468812073\n",
      "\tap@k: 0.942983740864\n",
      "Train:\n",
      "\tloss: 0.240990679388\n",
      "\tacc: 0.901683168317\n",
      "\tauc: 0.927496188348\n",
      "\tap@k: 0.825199266732\n",
      "Val:\n",
      "\tloss: 0.193896609964\n",
      "\tacc: 0.910495049505\n",
      "\tauc: 0.961849700326\n",
      "\tap@k: 0.943099306812\n",
      "Train:\n",
      "\tloss: 0.234102756877\n",
      "\tacc: 0.901881188119\n",
      "\tauc: 0.928295116454\n",
      "\tap@k: 0.886257128736\n",
      "Val:\n",
      "\tloss: 0.157576298419\n",
      "\tacc: 0.935445544554\n",
      "\tauc: 0.973158136237\n",
      "\tap@k: 0.947802121767\n",
      "Train:\n",
      "\tloss: 0.214912991813\n",
      "\tacc: 0.911089108911\n",
      "\tauc: 0.930662201008\n",
      "\tap@k: 0.799683611406\n",
      "Val:\n",
      "\tloss: 0.181380155393\n",
      "\tacc: 0.916237623762\n",
      "\tauc: 0.954201406405\n",
      "\tap@k: 0.930842056444\n",
      "Train:\n",
      "\tloss: 0.210384061337\n",
      "\tacc: 0.911386138614\n",
      "\tauc: 0.932884759684\n",
      "\tap@k: 0.91472867147\n",
      "Val:\n",
      "\tloss: 0.171667402646\n",
      "\tacc: 0.93900990099\n",
      "\tauc: 0.976046131793\n",
      "\tap@k: 0.982286988026\n",
      "Train:\n",
      "\tloss: 0.197167747516\n",
      "\tacc: 0.914356435644\n",
      "\tauc: 0.9323611532\n",
      "\tap@k: 0.832613495061\n",
      "Val:\n",
      "\tloss: 0.198179887875\n",
      "\tacc: 0.903762376238\n",
      "\tauc: 0.950273637112\n",
      "\tap@k: 0.919371868203\n",
      "Train:\n",
      "\tloss: 0.199519082196\n",
      "\tacc: 0.912772277228\n",
      "\tauc: 0.936473436176\n",
      "\tap@k: 0.893007672469\n",
      "Val:\n",
      "\tloss: 0.159129377267\n",
      "\tacc: 0.942475247525\n",
      "\tauc: 0.968144207252\n",
      "\tap@k: 0.970494844457\n",
      "Train:\n",
      "\tloss: 0.213377223493\n",
      "\tacc: 0.916633663366\n",
      "\tauc: 0.938315418868\n",
      "\tap@k: 0.795422137729\n",
      "Val:\n",
      "\tloss: 0.145053391392\n",
      "\tacc: 0.940792079208\n",
      "\tauc: 0.969370534853\n",
      "\tap@k: 0.944273629145\n",
      "Train:\n",
      "\tloss: 0.199669178653\n",
      "\tacc: 0.915247524752\n",
      "\tauc: 0.934644679331\n",
      "\tap@k: 0.850794001817\n",
      "Val:\n",
      "\tloss: 0.152329422129\n",
      "\tacc: 0.942376237624\n",
      "\tauc: 0.974512027233\n",
      "\tap@k: 0.960571707714\n",
      "Train:\n",
      "\tloss: 0.189171871248\n",
      "\tacc: 0.918217821782\n",
      "\tauc: 0.937445825048\n",
      "\tap@k: 0.853579979474\n",
      "Val:\n",
      "\tloss: 0.144228532662\n",
      "\tacc: 0.943168316832\n",
      "\tauc: 0.972677572914\n",
      "\tap@k: 0.979062046349\n",
      "Train:\n",
      "\tloss: 0.182348854164\n",
      "\tacc: 0.922079207921\n",
      "\tauc: 0.935328199259\n",
      "\tap@k: 0.861940975199\n",
      "Val:\n",
      "\tloss: 0.138051077493\n",
      "\tacc: 0.942376237624\n",
      "\tauc: 0.968692352083\n",
      "\tap@k: 0.935800243638\n",
      "Train:\n",
      "\tloss: 0.180495231877\n",
      "\tacc: 0.924059405941\n",
      "\tauc: 0.944934646751\n",
      "\tap@k: 0.902360469056\n",
      "Val:\n",
      "\tloss: 0.12670653093\n",
      "\tacc: 0.945643564356\n",
      "\tauc: 0.974109743161\n",
      "\tap@k: 0.955701529524\n",
      "Train:\n",
      "\tloss: 0.174275236767\n",
      "\tacc: 0.924257425743\n",
      "\tauc: 0.94288699731\n",
      "\tap@k: 0.918661749317\n",
      "Val:\n",
      "\tloss: 0.158442415194\n",
      "\tacc: 0.939108910891\n",
      "\tauc: 0.96593518194\n",
      "\tap@k: 0.939368372623\n",
      "Train:\n",
      "\tloss: 0.16992602379\n",
      "\tacc: 0.925643564356\n",
      "\tauc: 0.944997400311\n",
      "\tap@k: 0.929327829241\n",
      "Val:\n",
      "\tloss: 0.126631456808\n",
      "\tacc: 0.949108910891\n",
      "\tauc: 0.973350820445\n",
      "\tap@k: 0.950681405337\n",
      "Train:\n",
      "\tloss: 0.188008892064\n",
      "\tacc: 0.92099009901\n",
      "\tauc: 0.941375181156\n",
      "\tap@k: 0.916544194589\n",
      "Val:\n",
      "\tloss: 0.148593111026\n",
      "\tacc: 0.933366336634\n",
      "\tauc: 0.957475811254\n",
      "\tap@k: 0.901582504751\n",
      "Train:\n",
      "\tloss: 0.169512238137\n",
      "\tacc: 0.926831683168\n",
      "\tauc: 0.943741774722\n",
      "\tap@k: 0.878237273236\n",
      "Val:\n",
      "\tloss: 0.120787465147\n",
      "\tacc: 0.951881188119\n",
      "\tauc: 0.974867024635\n",
      "\tap@k: 0.965395571859\n",
      "Train:\n",
      "\tloss: 0.164610312015\n",
      "\tacc: 0.927425742574\n",
      "\tauc: 0.941330627958\n",
      "\tap@k: 0.871325320962\n",
      "Val:\n",
      "\tloss: 0.12182038509\n",
      "\tacc: 0.949603960396\n",
      "\tauc: 0.973977123895\n",
      "\tap@k: 0.952319734539\n",
      "Train:\n",
      "\tloss: 0.160096402127\n",
      "\tacc: 0.932673267327\n",
      "\tauc: 0.949954311942\n",
      "\tap@k: 0.896891651458\n",
      "Val:\n",
      "\tloss: 0.135676820697\n",
      "\tacc: 0.95099009901\n",
      "\tauc: 0.979570395398\n",
      "\tap@k: 0.978230013224\n",
      "Train:\n",
      "\tloss: 0.168113765876\n",
      "\tacc: 0.924653465347\n",
      "\tauc: 0.940686004447\n",
      "\tap@k: 0.907895901031\n",
      "Val:\n",
      "\tloss: 0.1451372324\n",
      "\tacc: 0.939405940594\n",
      "\tauc: 0.968208561157\n",
      "\tap@k: 0.973040233258\n",
      "Train:\n",
      "\tloss: 0.17020094917\n",
      "\tacc: 0.930198019802\n",
      "\tauc: 0.947246006822\n",
      "\tap@k: 0.882278694809\n",
      "Val:\n",
      "\tloss: 0.146816057409\n",
      "\tacc: 0.950792079208\n",
      "\tauc: 0.977914568328\n",
      "\tap@k: 0.962512156218\n",
      "Train:\n",
      "\tloss: 0.150411804442\n",
      "\tacc: 0.942772277228\n",
      "\tauc: 0.958496793174\n",
      "\tap@k: 0.95800121537\n",
      "Val:\n",
      "\tloss: 0.130098440387\n",
      "\tacc: 0.94702970297\n",
      "\tauc: 0.97214482719\n",
      "\tap@k: 0.978127238454\n",
      "Train:\n",
      "\tloss: 0.126294570394\n",
      "\tacc: 0.951683168317\n",
      "\tauc: 0.970837465791\n",
      "\tap@k: 0.966562522607\n",
      "Val:\n",
      "\tloss: 0.179394308258\n",
      "\tacc: 0.94900990099\n",
      "\tauc: 0.973541236905\n",
      "\tap@k: 0.997926835818\n",
      "Train:\n",
      "\tloss: 0.125131314657\n",
      "\tacc: 0.95297029703\n",
      "\tauc: 0.974949675205\n",
      "\tap@k: 0.989566704574\n",
      "Val:\n",
      "\tloss: 0.139450841947\n",
      "\tacc: 0.939603960396\n",
      "\tauc: 0.97236796916\n",
      "\tap@k: 0.998197518199\n",
      "Train:\n",
      "\tloss: 0.137391402957\n",
      "\tacc: 0.945544554455\n",
      "\tauc: 0.96868873141\n",
      "\tap@k: 0.982902041841\n",
      "Val:\n",
      "\tloss: 0.124569570747\n",
      "\tacc: 0.943663366337\n",
      "\tauc: 0.975720368644\n",
      "\tap@k: 0.996019123384\n",
      "Train:\n",
      "\tloss: 0.736763828627\n",
      "\tacc: 0.923663366337\n",
      "\tauc: 0.946409471595\n",
      "\tap@k: 0.895109108867\n",
      "Val:\n",
      "\tloss: 0.146191057451\n",
      "\tacc: 0.945544554455\n",
      "\tauc: 0.976053849131\n",
      "\tap@k: 0.963090013945\n",
      "Train:\n",
      "\tloss: 0.158178220857\n",
      "\tacc: 0.930594059406\n",
      "\tauc: 0.947113776868\n",
      "\tap@k: 0.996497398599\n",
      "Val:\n",
      "\tloss: 0.210631836665\n",
      "\tacc: 0.949900990099\n",
      "\tauc: 0.97583206993\n",
      "\tap@k: 0.943175928982\n",
      "Train:\n",
      "\tloss: 0.176045260689\n",
      "\tacc: 0.924257425743\n",
      "\tauc: 0.938555655952\n",
      "\tap@k: 0.897106354238\n",
      "Val:\n",
      "\tloss: 0.129363742736\n",
      "\tacc: 0.952079207921\n",
      "\tauc: 0.975856727692\n",
      "\tap@k: 0.945708386756\n",
      "Train:\n",
      "\tloss: 0.169582269587\n",
      "\tacc: 0.929108910891\n",
      "\tauc: 0.945368419798\n",
      "\tap@k: 0.902870773671\n",
      "Val:\n",
      "\tloss: 0.131877066511\n",
      "\tacc: 0.944752475248\n",
      "\tauc: 0.973886597915\n",
      "\tap@k: 0.904009288439\n",
      "Train:\n",
      "\tloss: 0.163230855424\n",
      "\tacc: 0.927425742574\n",
      "\tauc: 0.944326840415\n",
      "\tap@k: 0.911892955519\n",
      "Val:\n",
      "\tloss: 0.130970466898\n",
      "\tacc: 0.951386138614\n",
      "\tauc: 0.977620785371\n",
      "\tap@k: 0.936420074629\n",
      "Train:\n",
      "\tloss: 0.160380231349\n",
      "\tacc: 0.931782178218\n",
      "\tauc: 0.946738857264\n",
      "\tap@k: 0.920689474877\n",
      "Val:\n",
      "\tloss: 0.117374956779\n",
      "\tacc: 0.952574257426\n",
      "\tauc: 0.977914536236\n",
      "\tap@k: 0.960209499026\n",
      "Train:\n",
      "\tloss: 0.166460254694\n",
      "\tacc: 0.928217821782\n",
      "\tauc: 0.944161932844\n",
      "\tap@k: 0.899805019919\n",
      "Val:\n",
      "\tloss: 0.133788543816\n",
      "\tacc: 0.957425742574\n",
      "\tauc: 0.978853291245\n",
      "\tap@k: 0.926535465249\n",
      "Train:\n",
      "\tloss: 0.162389486167\n",
      "\tacc: 0.930396039604\n",
      "\tauc: 0.942746411699\n",
      "\tap@k: 0.906872210648\n",
      "Val:\n",
      "\tloss: 0.116633159678\n",
      "\tacc: 0.951584158416\n",
      "\tauc: 0.973189606185\n",
      "\tap@k: 0.964291445923\n",
      "Train:\n",
      "\tloss: 0.156789774983\n",
      "\tacc: 0.931089108911\n",
      "\tauc: 0.946084366182\n",
      "\tap@k: 0.889364332153\n",
      "Val:\n",
      "\tloss: 0.11181124522\n",
      "\tacc: 0.951287128713\n",
      "\tauc: 0.971431732082\n",
      "\tap@k: 0.936811059606\n",
      "Train:\n",
      "\tloss: 0.150740893696\n",
      "\tacc: 0.934059405941\n",
      "\tauc: 0.952031208486\n",
      "\tap@k: 0.916650343266\n",
      "Val:\n",
      "\tloss: 0.119758286506\n",
      "\tacc: 0.950297029703\n",
      "\tauc: 0.974772769016\n",
      "\tap@k: 0.966636930064\n",
      "Train:\n",
      "\tloss: 0.147101994428\n",
      "\tacc: 0.936930693069\n",
      "\tauc: 0.9484012028\n",
      "\tap@k: 0.93565554388\n",
      "Val:\n",
      "\tloss: 0.118728482239\n",
      "\tacc: 0.948514851485\n",
      "\tauc: 0.97713357029\n",
      "\tap@k: 0.955512190495\n",
      "Train:\n",
      "\tloss: 0.131862626152\n",
      "\tacc: 0.942178217822\n",
      "\tauc: 0.957243252113\n",
      "\tap@k: 0.953195612797\n",
      "Val:\n",
      "\tloss: 0.111234335622\n",
      "\tacc: 0.954356435644\n",
      "\tauc: 0.977901206459\n",
      "\tap@k: 0.949743574845\n",
      "Train:\n",
      "\tloss: 0.120109301711\n",
      "\tacc: 0.953465346535\n",
      "\tauc: 0.969891262714\n",
      "\tap@k: 0.958447873347\n",
      "Val:\n",
      "\tloss: 0.210298878674\n",
      "\tacc: 0.932475247525\n",
      "\tauc: 0.970336514865\n",
      "\tap@k: 0.948270471655\n",
      "Train:\n",
      "\tloss: 0.14724945187\n",
      "\tacc: 0.947425742574\n",
      "\tauc: 0.966018006293\n",
      "\tap@k: 0.883471364697\n",
      "Val:\n",
      "\tloss: 0.182580208631\n",
      "\tacc: 0.955148514851\n",
      "\tauc: 0.972700893862\n",
      "\tap@k: 0.954392199104\n",
      "Train:\n",
      "\tloss: 0.130909874239\n",
      "\tacc: 0.948316831683\n",
      "\tauc: 0.963324825611\n",
      "\tap@k: 0.940433709387\n",
      "Val:\n",
      "\tloss: 0.138816762065\n",
      "\tacc: 0.936534653465\n",
      "\tauc: 0.959151243135\n",
      "\tap@k: 0.917337048114\n",
      "Train:\n",
      "\tloss: 0.104356984868\n",
      "\tacc: 0.957920792079\n",
      "\tauc: 0.973354524466\n",
      "\tap@k: 0.969936694443\n",
      "Val:\n",
      "\tloss: 0.142159620293\n",
      "\tacc: 0.952772277228\n",
      "\tauc: 0.972508211544\n",
      "\tap@k: 0.96795574501\n",
      "Train:\n",
      "\tloss: 0.105476568082\n",
      "\tacc: 0.958316831683\n",
      "\tauc: 0.973147054241\n",
      "\tap@k: 0.968741184827\n",
      "Val:\n",
      "\tloss: 0.135625236955\n",
      "\tacc: 0.952376237624\n",
      "\tauc: 0.975323510201\n",
      "\tap@k: 0.997277354241\n",
      "Train:\n",
      "\tloss: 0.101740863636\n",
      "\tacc: 0.961683168317\n",
      "\tauc: 0.973715616275\n",
      "\tap@k: 0.952855917694\n",
      "Val:\n",
      "\tloss: 0.131649094678\n",
      "\tacc: 0.947821782178\n",
      "\tauc: 0.974669635921\n",
      "\tap@k: 0.957767701734\n",
      "Train:\n",
      "\tloss: 0.106572975983\n",
      "\tacc: 0.957326732673\n",
      "\tauc: 0.976581482933\n",
      "\tap@k: 0.934585709144\n",
      "Val:\n",
      "\tloss: 0.124579237404\n",
      "\tacc: 0.956435643564\n",
      "\tauc: 0.981181290687\n",
      "\tap@k: 0.961712244819\n",
      "Train:\n",
      "\tloss: 0.102396538623\n",
      "\tacc: 0.957524752475\n",
      "\tauc: 0.976111493558\n",
      "\tap@k: 0.947751972971\n",
      "Val:\n",
      "\tloss: 0.120264296343\n",
      "\tacc: 0.957623762376\n",
      "\tauc: 0.978735413654\n",
      "\tap@k: 0.935158670435\n",
      "Train:\n",
      "\tloss: 0.112484152937\n",
      "\tacc: 0.956435643564\n",
      "\tauc: 0.974930272388\n",
      "\tap@k: 0.958010392328\n",
      "Val:\n",
      "\tloss: 0.12382280715\n",
      "\tacc: 0.959900990099\n",
      "\tauc: 0.981089553365\n",
      "\tap@k: 0.959075229268\n",
      "Train:\n",
      "\tloss: 0.101604683576\n",
      "\tacc: 0.960495049505\n",
      "\tauc: 0.977892196328\n",
      "\tap@k: 0.963909937354\n",
      "Val:\n",
      "\tloss: 0.125189238379\n",
      "\tacc: 0.950693069307\n",
      "\tauc: 0.969954357237\n",
      "\tap@k: 0.924631347806\n",
      "Train:\n",
      "\tloss: 0.0997270778559\n",
      "\tacc: 0.96297029703\n",
      "\tauc: 0.979479518529\n",
      "\tap@k: 0.955490469129\n",
      "Val:\n",
      "\tloss: 0.106213087392\n",
      "\tacc: 0.959207920792\n",
      "\tauc: 0.981011432733\n",
      "\tap@k: 0.947924265732\n",
      "Train:\n",
      "\tloss: 0.0959118461196\n",
      "\tacc: 0.963564356436\n",
      "\tauc: 0.977988160153\n",
      "\tap@k: 0.942663517861\n",
      "Val:\n",
      "\tloss: 0.120734671661\n",
      "\tacc: 0.95495049505\n",
      "\tauc: 0.981418537316\n",
      "\tap@k: 0.945504406098\n",
      "Train:\n",
      "\tloss: 0.0902651044756\n",
      "\tacc: 0.962277227723\n",
      "\tauc: 0.979471346896\n",
      "\tap@k: 0.958565945069\n",
      "Val:\n",
      "\tloss: 0.123940662494\n",
      "\tacc: 0.960495049505\n",
      "\tauc: 0.982912975507\n",
      "\tap@k: 0.99715712214\n",
      "Train:\n",
      "\tloss: 0.0903576123419\n",
      "\tacc: 0.962871287129\n",
      "\tauc: 0.979774505677\n",
      "\tap@k: 0.984152353151\n",
      "Val:\n",
      "\tloss: 0.15959553574\n",
      "\tacc: 0.957128712871\n",
      "\tauc: 0.982249788156\n",
      "\tap@k: 0.983352916956\n",
      "Train:\n",
      "\tloss: 0.106467180966\n",
      "\tacc: 0.963465346535\n",
      "\tauc: 0.978507816687\n",
      "\tap@k: 0.977147296047\n",
      "Val:\n",
      "\tloss: 0.141956054686\n",
      "\tacc: 0.954851485149\n",
      "\tauc: 0.987094179923\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0965252493101\n",
      "\tacc: 0.962574257426\n",
      "\tauc: 0.980428123776\n",
      "\tap@k: 0.966523434766\n",
      "Val:\n",
      "\tloss: 0.133372546493\n",
      "\tacc: 0.955841584158\n",
      "\tauc: 0.980502909972\n",
      "\tap@k: 0.967641409703\n",
      "Train:\n",
      "\tloss: 0.114275426156\n",
      "\tacc: 0.96603960396\n",
      "\tauc: 0.981039162039\n",
      "\tap@k: 0.897105243537\n",
      "Val:\n",
      "\tloss: 0.200548183818\n",
      "\tacc: 0.958613861386\n",
      "\tauc: 0.984847190799\n",
      "\tap@k: 0.968917454045\n",
      "Train:\n",
      "\tloss: 0.0914686866621\n",
      "\tacc: 0.963762376238\n",
      "\tauc: 0.977469405906\n",
      "\tap@k: 0.948093823132\n",
      "Val:\n",
      "\tloss: 0.107495370501\n",
      "\tacc: 0.957623762376\n",
      "\tauc: 0.981159997308\n",
      "\tap@k: 0.946553373825\n",
      "Train:\n",
      "\tloss: 0.0822273600515\n",
      "\tacc: 0.968118811881\n",
      "\tauc: 0.981404468843\n",
      "\tap@k: 0.980803478051\n",
      "Val:\n",
      "\tloss: 0.105200744696\n",
      "\tacc: 0.96099009901\n",
      "\tauc: 0.98259201766\n",
      "\tap@k: 0.942337338344\n",
      "Train:\n",
      "\tloss: 0.079353089964\n",
      "\tacc: 0.967920792079\n",
      "\tauc: 0.980395606661\n",
      "\tap@k: 0.958574581153\n",
      "Val:\n",
      "\tloss: 0.106614695168\n",
      "\tacc: 0.962772277228\n",
      "\tauc: 0.982803400185\n",
      "\tap@k: 0.954199727178\n",
      "Train:\n",
      "\tloss: 0.0906769703697\n",
      "\tacc: 0.964752475248\n",
      "\tauc: 0.977872020689\n",
      "\tap@k: 0.939665379464\n",
      "Val:\n",
      "\tloss: 0.101797687476\n",
      "\tacc: 0.958415841584\n",
      "\tauc: 0.983067943499\n",
      "\tap@k: 0.971397664007\n",
      "Train:\n",
      "\tloss: 0.0824894559796\n",
      "\tacc: 0.968910891089\n",
      "\tauc: 0.980713388473\n",
      "\tap@k: 0.956565331974\n",
      "Val:\n",
      "\tloss: 0.103773313778\n",
      "\tacc: 0.952871287129\n",
      "\tauc: 0.976429893114\n",
      "\tap@k: 0.959595988452\n",
      "Train:\n",
      "\tloss: 0.0839373389506\n",
      "\tacc: 0.965445544554\n",
      "\tauc: 0.981177658134\n",
      "\tap@k: 0.972738768887\n",
      "Val:\n",
      "\tloss: 0.125386857231\n",
      "\tacc: 0.95900990099\n",
      "\tauc: 0.984656827939\n",
      "\tap@k: 0.982556331758\n",
      "Train:\n",
      "\tloss: 0.0923579967435\n",
      "\tacc: 0.96495049505\n",
      "\tauc: 0.979874783629\n",
      "\tap@k: 0.954000819429\n",
      "Val:\n",
      "\tloss: 0.11163987217\n",
      "\tacc: 0.957623762376\n",
      "\tauc: 0.975721783413\n",
      "\tap@k: 0.951005143555\n",
      "Train:\n",
      "\tloss: 0.0818520210934\n",
      "\tacc: 0.966831683168\n",
      "\tauc: 0.981312513048\n",
      "\tap@k: 0.948574413945\n",
      "Val:\n",
      "\tloss: 0.116661806803\n",
      "\tacc: 0.957326732673\n",
      "\tauc: 0.981018884497\n",
      "\tap@k: 0.97705040872\n",
      "Train:\n",
      "\tloss: 0.0833695504276\n",
      "\tacc: 0.968415841584\n",
      "\tauc: 0.983108825838\n",
      "\tap@k: 0.942534009631\n",
      "Val:\n",
      "\tloss: 0.127216404446\n",
      "\tacc: 0.960198019802\n",
      "\tauc: 0.984716399754\n",
      "\tap@k: 0.990265810308\n",
      "Train:\n",
      "\tloss: 0.0791293752607\n",
      "\tacc: 0.968712871287\n",
      "\tauc: 0.983135401573\n",
      "\tap@k: 0.961953291832\n",
      "Val:\n",
      "\tloss: 0.120056423622\n",
      "\tacc: 0.955148514851\n",
      "\tauc: 0.979628862217\n",
      "\tap@k: 0.966755368679\n",
      "Train:\n",
      "\tloss: 0.0744732004699\n",
      "\tacc: 0.968415841584\n",
      "\tauc: 0.981384876164\n",
      "\tap@k: 0.974844101898\n",
      "Val:\n",
      "\tloss: 0.110381783306\n",
      "\tacc: 0.965148514851\n",
      "\tauc: 0.986506315738\n",
      "\tap@k: 0.974028262488\n",
      "Train:\n",
      "\tloss: 0.117475050829\n",
      "\tacc: 0.964554455446\n",
      "\tauc: 0.980591887246\n",
      "\tap@k: 0.95010628633\n",
      "Val:\n",
      "\tloss: 0.137054418626\n",
      "\tacc: 0.96198019802\n",
      "\tauc: 0.983975947079\n",
      "\tap@k: 0.965563095287\n",
      "Train:\n",
      "\tloss: 0.0930463289624\n",
      "\tacc: 0.964158415842\n",
      "\tauc: 0.981414324409\n",
      "\tap@k: 0.995189851542\n",
      "Val:\n",
      "\tloss: 0.131389605297\n",
      "\tacc: 0.958910891089\n",
      "\tauc: 0.979557810267\n",
      "\tap@k: 0.978635710218\n",
      "Train:\n",
      "\tloss: 0.0817048536728\n",
      "\tacc: 0.971287128713\n",
      "\tauc: 0.985953009402\n",
      "\tap@k: 0.99476684351\n",
      "Val:\n",
      "\tloss: 0.0992161391513\n",
      "\tacc: 0.959207920792\n",
      "\tauc: 0.981606299844\n",
      "\tap@k: 0.937782221896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Train:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Val:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.103771585962\n",
      "\tacc: 0.957944\n",
      "\tauc: 0.980221927839\n",
      "\tap@k: 0.967505287565\n",
      "\n",
      "AUC:\n",
      "\tОтличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "\tОтличный результат! (good)\n",
      "\n",
      "Average precision at K:\n",
      "\tОтличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print(\"Scores:\")\n",
    "print('\\tloss:',b_loss/b_c)\n",
    "print('\\tacc:',final_accuracy)\n",
    "print('\\tauc:',final_auc)\n",
    "print('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
